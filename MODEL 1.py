# -*- coding: utf-8 -*-
"""Untitled45.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FSWj-pRcj4b2wRnSwYC7wrvq6Ua5PiUq
"""

'''2. Text Classification

Datasets  Newsgroups are easily accessible and well-suited for topic classification.
Random Forest and SVM are robust baseline algorithms, and Particle Swarm Optimization (PSO) enhances feature selection and hyperparameter tuning effectively.
High interpretability and explainability in results, which are valuable for research papers.
Easier to benchmark against traditional ML models and advanced neural networks.'''

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from pyswarm import pso
import matplotlib.pyplot as plt
import seaborn as sns

# Load Dataset (20 Newsgroups)
data = fetch_20newsgroups(subset='all', categories=['rec.autos', 'sci.med', 'comp.graphics', 'talk.politics.mideast'], remove=('headers', 'footers', 'quotes'))
X = data.data
y = data.target

# Preprocessing
vectorizer = TfidfVectorizer(max_features=2000)
X = vectorizer.fit_transform(X).toarray()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Fitness Function for PSO
best_acc = 0
best_params = None

def fitness_function(params):
    n_estimators = int(params[0])
    max_depth = int(params[1])
    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    return -acc  # Negative because PSO minimizes

# PSO Optimization
lb = [10, 5]  # Lower bound for n_estimators, max_depth
ub = [100, 20]  # Upper bound for n_estimators, max_depth
opt_params, opt_score = pso(fitness_function, lb, ub, swarmsize=5, maxiter=10)

### Text Classification with Random Forest + PSO

# Final Model with Best Params
best_model = RandomForestClassifier(n_estimators=int(opt_params[0]), max_depth=int(opt_params[1]), random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

# Results and Visualization
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

# Model Evaluation for Base Model
base_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
base_model.fit(X_train, y_train)
y_pred_base = base_model.predict(X_test)
acc_base = accuracy_score(y_test, y_pred_base)
f1_base = f1_score(y_test, y_pred_base, average='weighted')

# Model Evaluation for Optimized Model (with PSO)
acc_optimized = accuracy_score(y_test, y_pred)
f1_optimized = f1_score(y_test, y_pred, average='weighted')

# Printing accuracy and f1 score for base and optimized models
print("Base Model - Accuracy: {:.4f}, F1 Score: {:.4f}".format(acc_base, f1_base))
print("Optimized Model - Accuracy: {:.4f}, F1 Score: {:.4f}".format(acc_optimized, f1_optimized))

# Printing accuracy and f1 score for base and optimized models
print("Base Model - Accuracy: {:.4f}, F1 Score: {:.4f}".format(acc_base, f1_base))
print("Optimized Model - Accuracy: {:.4f}, F1 Score: {:.4f}".format(acc_optimized, f1_optimized))

# Visual Comparison
metrics = ['Accuracy', 'F1 Score']
base_metrics = [acc_base, f1_base]
optimized_metrics = [acc_optimized, f1_optimized]

# Bar Plot for comparison
fig, ax = plt.subplots(figsize=(8, 5))
width = 0.3
x = np.arange(len(metrics))
ax.bar(x - width/2, base_metrics, width, label='Base Model', color='b')
ax.bar(x + width/2, optimized_metrics, width, label='Optimized Model', color='g')

ax.set_ylabel('Scores')
ax.set_title('Comparison of Base and Optimized Model')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()

plt.show()

# Confusion Matrix Comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Base Model Confusion Matrix
cm_base = confusion_matrix(y_test, y_pred_base)
sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Base Model Confusion Matrix')

# Optimized Model Confusion Matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title('Optimized Model Confusion Matrix')

plt.tight_layout()
plt.show()



#visual 2 (ROC-AUC CURVE)

from sklearn.preprocessing import label_binarize
from sklearn.metrics import precision_recall_curve, roc_curve, auc

# Binarizing the labels for multi-class evaluation
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
y_pred_base_prob_bin = base_model.predict_proba(X_test)
y_pred_optimized_prob_bin = best_model.predict_proba(X_test)

# Precision-Recall Curve for each class
def plot_multiclass_precision_recall(y_test_bin, y_pred_prob_bin, model_label):
    plt.figure(figsize=(8, 6))
    for i in range(y_test_bin.shape[1]):
        precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_prob_bin[:, i])
        plt.plot(recall, precision, label=f'Class {i} {model_label} PR Curve')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Multi-Class Precision-Recall Curves - {model_label}')
    plt.legend(loc='best')
    plt.show()

# ROC Curve for each class
def plot_multiclass_roc_curve(y_test_bin, y_pred_prob_bin, model_label):
    plt.figure(figsize=(8, 6))
    for i in range(y_test_bin.shape[1]):
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob_bin[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'Class {i} {model_label} ROC Curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Multi-Class ROC Curves - {model_label}')
    plt.legend(loc='best')
    plt.show()

# Plot Precision-Recall and ROC Curves for both base and optimized models

plot_multiclass_precision_recall(y_test_bin, y_pred_base_prob_bin, "Base Model")

plot_multiclass_precision_recall(y_test_bin, y_pred_optimized_prob_bin, "Optimized Model")

plot_multiclass_roc_curve(y_test_bin, y_pred_base_prob_bin, "Base Model")

plot_multiclass_roc_curve(y_test_bin, y_pred_optimized_prob_bin, "Optimized Model")

# Feature Importance Visualization for both models
def plot_feature_importance(model, vectorizer, model_label):
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    features = np.array(vectorizer.get_feature_names_out())

    top_n = 20
    plt.figure(figsize=(10, 6))
    plt.barh(range(top_n), importances[indices[:top_n]], align="center")
    plt.yticks(range(top_n), features[indices[:top_n]])
    plt.xlabel('Feature Importance')
    plt.title(f'Top {top_n} Important Features for {model_label}')
    plt.gca().invert_yaxis()
    plt.show()

# Feature Importance for Base and Optimized Model
plot_feature_importance(base_model, vectorizer, "Base Model")

plot_feature_importance(best_model, vectorizer, "Optimized Model")





##LOSS

# Adding loss tracking during PSO optimization

# List to store accuracy and loss (error) for each PSO iteration
train_losses = []
val_losses = []

# Modify fitness function to track loss during PSO optimization
def fitness_function_with_loss(params):
    n_estimators = int(params[0])
    max_depth = int(params[1])
    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)

    # Training the model
    model.fit(X_train, y_train)

    # Make predictions on training and validation sets
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_test)

    # Calculate accuracy for training and validation sets
    train_acc = accuracy_score(y_train, y_train_pred)
    val_acc = accuracy_score(y_test, y_val_pred)

    # Store losses (1 - accuracy) for each iteration
    train_losses.append(1 - train_acc)
    val_losses.append(1 - val_acc)

    return -(val_acc)  # We want to maximize validation accuracy

# PSO Optimization with loss tracking
opt_params, opt_score = pso(fitness_function_with_loss, lb, ub, swarmsize=5, maxiter=10)

# Plotting Training and Validation Losses (Error Rates)
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='b')
plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='g')
plt.xlabel('Iteration')
plt.ylabel('Loss (1 - Accuracy)')
plt.title('Training and Validation Losses Over PSO Iterations')
plt.legend()
plt.grid(True)
plt.show()

#evaluating the final optimized model
final_model = RandomForestClassifier(n_estimators=int(opt_params[0]), max_depth=int(opt_params[1]), random_state=42)
final_model.fit(X_train, y_train)
y_pred_final = final_model.predict(X_test)

# Final Evaluation Metrics
final_acc = accuracy_score(y_test, y_pred_final)
final_f1 = f1_score(y_test, y_pred_final, average='weighted')

print(f"Optimized Model - Accuracy: {final_acc:.4f}, F1 Score: {final_f1:.4f}")

#LOSS VISUAL 2

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score





# Initialize lists to store training and validation accuracies and losses
train_accuracies = []
val_accuracies = []
train_losses = []
val_losses = []

# Modify the fitness function to track training and validation accuracy
def fitness_function_with_loss_tracking(params):
    n_estimators = int(params[0])
    max_depth = int(params[1])
    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions on both the training and validation sets
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_test)

    # Calculate training and validation accuracy
    train_acc = accuracy_score(y_train, y_train_pred)
    val_acc = accuracy_score(y_test, y_val_pred)

    # Append accuracies to the lists
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    # Calculate training and validation losses (1 - accuracy)
    train_losses.append(1 - train_acc)
    val_losses.append(1 - val_acc)

    # Return negative validation accuracy for PSO optimization
    return -(val_acc)





# Perform PSO optimization with loss tracking
#opt_params, opt_score = pso(fitness_function_with_loss_tracking, lb, ub, swarmsize=5, maxiter=10)

# Plot the Training and Validation Accuracy/Loss Graph
plt.figure(figsize=(12, 6))

# Plot Training and Validation Accuracies
plt.subplot(1, 2, 1)
plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy', color='b')
plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='g')
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)

# Plot Training and Validation Losses (1 - Accuracy)
plt.subplot(1, 2, 2)
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='b')
plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='g')
plt.xlabel('Iteration')
plt.ylabel('Loss (1 - Accuracy)')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()



